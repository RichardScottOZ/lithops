# Flow


FunctionExecutor class is responsible for orchestrating the computation in Lithops. One FunctionExecutor object is instantiated prior to any use of Lithops. Its initialization includes these important steps: 1. It sets up the workers (depending on the specific compute backend), such as constructing docker images, defining IBM Cloud Functions, etc. This step may not include actually creating the workers, as this may be done automatically by the backend on-demand. 2. It defines a bucket in object storage (depending on the storage backend) in which each job will store job and call data (prior to computation) and results (when computation is complete). 3. It creates a FunctionInvoker object, which is responsible for executing a job as a set of independent per-worker calls.

Compute jobs are created in the functions of the job module (see chart above), invoked from the respective API method of FunctionExecutor. Map jobs are created in create_map_job() and reduce jobs in create_reduce_job(). The flow in both functions is quite similar. First, data is partitioned, with the intention of each partition be processed by one worker. For map jobs, this is done by invoking the create_partitions() function of the partitioner module, yielding a partition map.

For reduce jobs, Lithops currently supports two modes: reduce per object, where each object is processed by a reduce function, and global (default) reduce, where all data is processed by a single reduce function. Respectively, data is partitioned as either one partition per storage object, or one global partition with all data. This process yields a partition map similar to map jobs. Additionally, create_reduce_job() wraps the reduce function in a special wrapper function that forces waiting for data before the actual reduce function is invoked. This is because reduce jobs follow map jobs, so the output of the map jobs needs to finish before reduce can run.

Eventually, both functions of create_map_job() and create_reduce_job() end up calling _create_job() which is the main flow of creating a job, described in high-level below: 1. A job_description record is defined for the job (and is eventually returned from all job creation functions) 2. The partition map and the data processing function (that processes a single partition in either map or reduce jobs) are each pickled (serialized) into a byte sequence. 3. The pickled partition map is stored in the object storage bucket, under agg_data_key object 4. The pickled processing function and its module dependencies are stored in the same bucket under func_key object

Once job creation is done and the job_description record for the new job is returned to the FunctionExecutor object, it proceeds to execute the job by calling run() method of its FunctionInvoker instance. This triggers the following flow: 1. The job is executed as a set of independent calls (invocations) that are submitted to a ThreadPoolExecutor object (thread pool size is defined by configuration). This means call invocation is concurrent from the start. 2. Each call executes first a call to an internal invoke() function defined inside FunctionInvoker.run(), which builds a payload (parameter) as a single dictionary with all the data the call needs. The call data includes copy of some of the job_description data as well as some specific data for the call such as: * call_id (integer ranging from 0 to total_calls - 1) * data_byte_range - defines the specific partition for this call, as defined by the partitioner during job creation * output_key - specific storage object (in the bucket) for computation output * status_key - specific storage object (in the bucket) for computation logs 3. Invocation proceeds to Compute.invoke(), which adds a retry mechanism for the current call, with random delays between retries (all configurable). 4. Invocation proceeds to ComputeBackend.invoke(). Further execution depends on the compute backend: * On IBM Cloud Functions, invoke() is performed as a standard non-blocking action invocation, with the payload being included as a single JSON parameter. * On Knative Serving, invoke() is performed as an HTTP POST request delivered over a connection that lasts for the entire time of the computation. * On a localhost (your laptop), invoke() is performed as posting the call on a queue. A master process continuously pulls calls from the queue and dispatches them onto processes from a pool of configurable size. * On a Docker cloud, invoke() is performed similar to localhost above, except the processes in the pool controlled by the master further delegate execution to a Docker container they create. 5. When computation completes, each call commits the result to object storage in the configured bucket under output_key object 6. Each invoke() returns a ResponseFuture object, which is a future object to wait on for the computed result of each call 7. A list of ResponseFuture objects returned by FunctionInvoker.run() is stored in the FunctionExecutor object and also returned by its respective method for map [+reduce] job. Later calls to wait() or get_result() can be used to wait for job completion and retrieve the results, respectively.